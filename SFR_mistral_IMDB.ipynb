{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64198-4c2a-4b11-b61a-1d420b327c10",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py311 (Python 3.11.10)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py311 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ad6ed-2ac2-451d-8d9a-419fc7e79a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1679b-f1bb-440f-a1ee-3c22957d9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d4243-aa33-431f-b80f-f81d1014e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load model and tokenizer\n",
    "tok = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral', cache_dir=\"./SFR_Mistral/\", torch_dtype=torch.float16)\n",
    "model = AutoModel.from_pretrained('Salesforce/SFR-Embedding-Mistral', cache_dir=\"./SFR_Mistral/\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# no special token\n",
    "# tok = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral', cache_dir=\"/SFR_Mistral/\", torch_dtype=torch.float16, add_special_tokens=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab582a-3525-4ef6-a24c-023b2c7a4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa4d83-a5f5-4088-8ea0-33b8859b7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0bda5-cceb-43ea-ae53-1ce3976d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from struct import pack, unpack\n",
    "import lzma\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "\n",
    "tconst_title = {}\n",
    "count = 0\n",
    "with open(\"./datasets/title.basics.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        if count > 0:\n",
    "            \n",
    "            tconst_title[row[0]] = row[2]\n",
    "            \n",
    "        count += 1\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e974ef0-b964-4022-97ac-48ef2f7651d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "name_tt = {}\n",
    "nconst_name = []\n",
    "nconst_profession = []\n",
    "nconst_embedding = []\n",
    "\n",
    "with open(\"./datasets/name.basics.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    counter = 0\n",
    "    \n",
    "    for row in rd:\n",
    "        if counter > 1000:\n",
    "            break\n",
    "        if counter > 0:            \n",
    "            prompt = row[1] +\" was born in \"+row[2]+\", \"+\"and died in \"+row[3]+\". He/She's primary professions are \"+', '.join(map(str, row[4].split(\",\")))+\".\"\n",
    "            tts = row[5].split(\",\")\n",
    "            prompt += \" He/She is known for movies:\"\n",
    "            for t in tts:\n",
    "                if t == tts[-1]:\n",
    "                    prompt += \" '\" + tconst_title[t]+\"'.\"\n",
    "                else:\n",
    "                    prompt += \" '\" + tconst_title[t]+\"',\"\n",
    "\n",
    "            max_length = 526\n",
    "            input_texts = prompt\n",
    "            batch_dict = tok(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch_dict)\n",
    "                embedding = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask']).to(\"cpu\")\n",
    "        \n",
    "            nconst_embedding.append(embedding[0])\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd842d96-abd2-4aa2-9efc-3cfc8a96676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83685c3b-fad6-4e72-bc1a-e9eee362e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nconst_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dde160-4545-4d0e-b940-c6f94c2ccf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFR mistral\n",
    "def get_relevant_documents(nconst_embeddings, query, topK, passages, model, tok):\n",
    "    batch_dict = tok(query, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = last_token_pool(outputs.last_hidden_state.to(\"cpu\"), batch_dict['attention_mask'].to(torch.device('cpu'))).to(\"cpu\")\n",
    "    \n",
    "    query_embedding = np.array(F.normalize(embeddings, p=2, dim=1)[0])\n",
    "    docs_embeddings = np.array(F.normalize(nconst_embeddings, p=2, dim=1))\n",
    "\n",
    "    # get cos similarity\n",
    "    logits = np.sum(query_embedding[None, :] * docs_embeddings, axis=-1)\n",
    "    top_k = logits.argsort()[-topK:][::-1]\n",
    "    print(\"top_k: \", top_k)\n",
    "    top_k_docs = [passages[i] for i in top_k]\n",
    "\n",
    "    return top_k_docs, top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02c992-0f54-49b7-a796-25c3e3f86764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_success_anotherrr(target_passage_id, top_k):\n",
    "    if target_passage_id in top_k:\n",
    "        if top_k[0] == target_passage_id:\n",
    "            return 2\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3480141-d4dd-4a29-a452-d863c74484da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd425",
   "metadata": {},
   "source": [
    "# Get retrival accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878067d-1880-4f0d-a78e-2d9e9e032a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DGD.help_functions import get_embedding, perturb_sentence, rank_tokens_by_importance, get_initial_ids, user_prompt_generation, check_success, check_MSEloss, get_first_output_token\n",
    "from DGD.DGD import get_optimized_prefix_embedding\n",
    "import random\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Set dataset\n",
    "dataset = \"imdb\"\n",
    "\n",
    "iterations = 20\n",
    "initial_firstk = 5\n",
    "\n",
    "results_store_dct = {}\n",
    "\n",
    "for i in range(0, len(prompts)):\n",
    "    print(\"----------> prompt index: \", i)\n",
    "    if i not in results_store_dct:\n",
    "        results_store_dct[i] = {}\n",
    "\n",
    "    user_query_id = i\n",
    "    target_passage_id = i\n",
    "    \n",
    "    prompt_text = prompts[i]\n",
    "    target_passage = prompts[i]\n",
    "\n",
    "    object = \"\"\n",
    "    query_mode = \"for_objects\"\n",
    "    user_query = user_prompt_generation(prompt_text, object, dataset, query_mode)\n",
    "\n",
    "    print(\"----------> user_query: \", user_query)    \n",
    "    list_length = 1\n",
    "    initial_ids = [random.randint(0, 32000) for _ in range(list_length)]\n",
    "    intital_prefix = tok.decode(initial_ids)\n",
    "\n",
    "    initial_ids = []\n",
    "\n",
    "    print(\"----------> intital_prefix: \", intital_prefix)\n",
    "\n",
    "    topk_result = 10\n",
    "\n",
    "    retrieved_docs, top_k = get_relevant_documents(torch.stack(nconst_embedding), user_query, topk_result, prompts, model, tok)\n",
    "\n",
    "    is_success = check_success_anotherrr(target_passage_id, top_k)\n",
    "    if is_success > 0:\n",
    "        print(\"----------> initial prefix success\")\n",
    "\n",
    "    token_ids = initial_ids\n",
    "    optimized_prefix = intital_prefix\n",
    "    results_store_dct[i][\"user_query_id\"] = i\n",
    "    results_store_dct[i][\"target_passage_id\"] = len(prompts) - i - 1\n",
    "    results_store_dct[i][\"prompt_text\"] = prompt_text\n",
    "    results_store_dct[i][\"target_passage\"] = target_passage\n",
    "    results_store_dct[i][\"token_ids\"] = token_ids\n",
    "    results_store_dct[i][\"optimized_prefix\"] = optimized_prefix\n",
    "    results_store_dct[i][\"is_success\"] = is_success\n",
    "\n",
    "    print(\"----------> optimized_prefix: \", optimized_prefix)\n",
    "    print(\"----------> user_query_text: \", prompt_text)\n",
    "    print(\"----------> target_passage: \", target_passage)\n",
    "\n",
    "    print(\"----------> is_success: \", is_success)\n",
    "    print(\"---------------> End <--------------\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917d1b8-7512-4132-bed0-7deb1db8944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = -1\n",
    "counter_success = 0\n",
    "counter = 0\n",
    "total = 0\n",
    "success_list = []\n",
    "for json_str in [results_store_dct]:\n",
    "    for index in json_str:\n",
    "        if 'is_success' not in json_str[index].keys():\n",
    "            continue\n",
    "        if json_str[index]['is_success'] > 0:\n",
    "            success_list.append(index)\n",
    "            counter += 1\n",
    "            if json_str[index]['is_success'] > 1:\n",
    "                counter_success += 1\n",
    "            prompt_text = json_str[index][\"prompt_text\"]\n",
    "            token_ids = json_str[index][\"token_ids\"]\n",
    "            \n",
    "        total += 1\n",
    "counter / total, counter_success / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461998e6",
   "metadata": {},
   "source": [
    "# Train adversial prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e184bd4-412d-48e7-8138-f7472ecb9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_embeddings = F.normalize(torch.stack(nconst_embedding), p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DGD.help_functions import get_embedding, perturb_sentence, rank_tokens_by_importance, get_initial_ids, user_prompt_generation, check_success, check_MSEloss, get_first_output_token, get_neighbor_ids, check_success, get_relevant_documents\n",
    "from DGD.DGD import get_optimized_prefix_embedding\n",
    "import random\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\")\n",
    "model.eval()\n",
    "MODEL_NAME = \"mistralai\"\n",
    "\n",
    "# Set dataset\n",
    "dataset = \"imdb\"\n",
    "query_mode = \"for_objects\"\n",
    "\n",
    "iterations = 500\n",
    "initial_firstk = 10\n",
    "\n",
    "results_store_dct = {}\n",
    "\n",
    "for i in range(208, len(prompts)):\n",
    "    print(\"----------> prompt index: \", i)\n",
    "    if i not in results_store_dct:\n",
    "        results_store_dct[i] = {}\n",
    "\n",
    "    user_prompt_id = i\n",
    "    prompt_text = prompts[i]\n",
    "    object = \"\"\n",
    "    user_prompt = user_prompt_generation(prompt_text, object, dataset, query_mode)\n",
    "\n",
    "    print(\"----------> user_prompt: \", user_prompt)\n",
    "\n",
    "    search_range = random.randint(20, 1000)\n",
    "    target_prompt_id = get_neighbor_ids(user_prompt_id, user_prompt, search_range, model, tok, nconst_embedding)\n",
    "\n",
    "    print(\"----------> user_prompt_id: \", user_prompt_id)\n",
    "    print(\"----------> prompt_text: \", prompt_text)\n",
    "    print(\"----------> target_prompt_id: \", target_prompt_id)\n",
    "    \n",
    "    target_text = prompts[target_prompt_id]\n",
    "\n",
    "    print(\"----------> target_text: \", target_text)\n",
    "\n",
    "    initial_firstk = 10\n",
    "    important_tokens = rank_tokens_by_importance(target_text, model, tok)\n",
    "    initial_ids = get_initial_ids(important_tokens, target_text, initial_firstk, MODEL_NAME, model, tok)\n",
    "    intital_prefix = tok.decode(initial_ids.tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    print(\"----------> intital_prefix: \", intital_prefix)\n",
    "\n",
    "    topk_result = 10\n",
    "    top_k = get_relevant_documents(torch.stack(nconst_embedding), intital_prefix+user_prompt, topk_result, model, tok)\n",
    "    is_success = check_success(target_prompt_id, user_prompt_id, top_k)\n",
    "    \n",
    "    if is_success > 1:\n",
    "        token_ids = initial_ids.tolist()\n",
    "        optimized_prefix = intital_prefix\n",
    "        print(\"----------> initial prefix success\")\n",
    "    else:\n",
    "        print(\"----------> optimized prompt index: \", i)\n",
    "        \n",
    "        thredhold = 1\n",
    "        token_ids_tensor, optimized_prefix, loss_list = get_optimized_prefix_embedding(MODEL_NAME, passages_embeddings[user_prompt_id], passages_embeddings[target_prompt_id], user_prompt, prompt_text, target_text, initial_ids, iterations, user_prompt_id, target_prompt_id, topk_result, model, tok, nconst_embedding, thredhold)\n",
    "        token_ids = token_ids_tensor[0].tolist()\n",
    "        top_k = get_relevant_documents(torch.stack(nconst_embedding), optimized_prefix+user_prompt, topk_result, model, tok)\n",
    "        is_success = check_success(target_prompt_id, user_prompt_id, top_k)\n",
    "        \n",
    "    results_store_dct[i][\"user_prompt_id\"] = i\n",
    "    results_store_dct[i][\"user_prompt\"] = user_prompt\n",
    "    results_store_dct[i][\"target_prompt_id\"] = target_prompt_id\n",
    "    results_store_dct[i][\"prompt_text\"] = prompt_text\n",
    "    results_store_dct[i][\"target_text\"] = target_text\n",
    "    results_store_dct[i][\"token_ids\"] = token_ids\n",
    "    results_store_dct[i][\"optimized_prefix\"] = optimized_prefix\n",
    "    results_store_dct[i][\"is_success\"] = is_success\n",
    "    results_store_dct[i][\"search_range\"] = search_range\n",
    "    results_store_dct[i][\"topk_result\"] = topk_result\n",
    "\n",
    "    print(\"----------> optimized_prefix: \", optimized_prefix)\n",
    "    print(\"----------> user_prompt_text: \", prompt_text)\n",
    "    print(\"----------> target_text: \", target_text)\n",
    "\n",
    "    print(\"----------> is_success: \", is_success)\n",
    "    print(\"---------------> End <--------------\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    with open('./results/SFR_Mistral_IMDB.pkl', 'wb') as file:\n",
    "        pickle.dump(results_store_dct, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
